{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-30T10:18:31.698875Z",
     "iopub.status.busy": "2025-12-30T10:18:31.698587Z",
     "iopub.status.idle": "2025-12-30T10:21:28.561659Z",
     "shell.execute_reply": "2025-12-30T10:21:28.560988Z",
     "shell.execute_reply.started": "2025-12-30T10:18:31.698854Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.7/208.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m846.0/846.0 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.4.0 which is incompatible.\n",
      "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.4.0 which is incompatible.\n",
      "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.4.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.0 which is incompatible.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "ydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.4.0 which is incompatible.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.8.0 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.8.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.4.0 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pyannote-metrics 4.0.0 requires numpy>=2.2.2, but you have numpy 1.26.4 which is incompatible.\n",
      "pyannote-core 6.0.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.8.0 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.8.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mâœ… Installation complete! Run main code next.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "2025-12-30 10:21:00.841418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767090061.207307      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767090061.306148      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==================== INSTALLATION (Cell 1) ====================\n",
    "!pip install -q openai-whisper pydub\n",
    "!pip install -q pyannote.audio==3.1.1\n",
    "!pip install -q numpy==1.26.4\n",
    "\n",
    "print(\"âœ… Installation complete! Run main code next.\")\n",
    "\n",
    "# ==================== MAIN CODE (Cell 2) ====================\n",
    "\n",
    "import whisper\n",
    "import torch\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import os\n",
    "import json\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ“¦ Libraries loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T10:21:39.280275Z",
     "iopub.status.busy": "2025-12-30T10:21:39.279514Z",
     "iopub.status.idle": "2025-12-30T10:24:31.254602Z",
     "shell.execute_reply": "2025-12-30T10:24:31.253815Z",
     "shell.execute_reply.started": "2025-12-30T10:21:39.280245Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸  Configuration:\n",
      "   Translation: Whisper large\n",
      "   Diarization: Pyannote 3.1 (DER: 11.2%)\n",
      "   Summarization: google/flan-t5-large\n",
      "\n",
      "ğŸš€ Starting complete meeting analysis pipeline...\n",
      "\n",
      "======================================================================\n",
      "ğŸ™ï¸  COMPLETE MEETING ANALYSIS PIPELINE\n",
      "======================================================================\n",
      "\n",
      "ğŸ† Best-in-class models:\n",
      "   1. Whisper Large - Translation\n",
      "   2. Pyannote 3.1 - Speaker Diarization (11.2% DER)\n",
      "   3. FLAN-T5-Large - Comprehensive Summarization\n",
      "\n",
      "ğŸ“‹ Pipeline stages:\n",
      "   Stage 1: Translate audio to English\n",
      "   Stage 2: Identify speakers with voice embeddings\n",
      "   Stage 3: Merge translation with speaker labels\n",
      "   Stage 4: Format readable transcript\n",
      "   Stage 5: Generate comprehensive summary\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Loading audio: /kaggle/input/eng-hinbi-marathi-mix-audio/Random recording for ReqGen.m4a\n",
      "â±ï¸  Duration: 2.60 minutes\n",
      "\n",
      "======================================================================\n",
      "STEP 1: TRANSLATION WITH WHISPER LARGE\n",
      "======================================================================\n",
      "ğŸ’» Device: cuda\n",
      "ğŸ¤ Loading Whisper large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.88G/2.88G [00:13<00:00, 223MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Translating to English...\n",
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: English\n",
      "[00:00.000 --> 00:13.840]  Hello everyone, good morning. I hope I am audible to everyone. Today's topic of our discussion will be on business card detection.\n",
      "[00:13.840 --> 00:26.400]  Right. So the agenda in discussing this project is that we have to detect the business card from a given video.\n",
      "[00:28.360 --> 00:33.620]  Which means that in that video, we will be showing the business card.\n",
      "[00:34.040 --> 00:40.400]  The person who has that business card will be showing it in the video, you will see that.\n",
      "[00:40.460 --> 00:41.100]  And then,\n",
      "[00:41.580 --> 00:41.900]  and then,\n",
      "[00:42.640 --> 00:45.640]  I will be showing you the details in detail.\n",
      "[00:45.640 --> 00:50.620]  So what this means is that we will be showing all the details on that business card,\n",
      "[00:50.900 --> 00:58.040]  like the name of the card holder or the name of the business person,\n",
      "[00:58.540 --> 01:05.220]  the name of the card, the company in which he is working,\n",
      "[01:05.460 --> 01:08.880]  the designation the person is at,\n",
      "[01:10.400 --> 01:11.780]  the address of the company,\n",
      "[01:11.900 --> 01:12.780]  or the office,\n",
      "[01:13.020 --> 01:14.520]  the phone number,\n",
      "[01:14.720 --> 01:15.700]  the email address.\n",
      "[01:16.040 --> 01:19.560]  So details like these needs to be extracted from the video.\n",
      "[01:20.080 --> 01:26.040]  So we need to discuss on how can we do it.\n",
      "[01:27.180 --> 01:31.680]  There are many ways that you can do normal Python coding,\n",
      "[01:32.040 --> 01:34.400]  or you can use plain code.\n",
      "[01:35.900 --> 01:38.980]  Apart from that, you can also use AI or deep learning models.\n",
      "[01:38.980 --> 01:44.120]  You can use the same business card to detect video or image.\n",
      "[01:47.580 --> 01:51.900]  And then after that, we will apply for information extraction,\n",
      "[01:52.380 --> 01:54.760]  for any modules or models that we have,\n",
      "[01:54.760 --> 01:56.000]  we will use that for this.\n",
      "[01:56.920 --> 01:57.480]  Correct?\n",
      "[01:57.700 --> 01:58.260]  Okay.\n",
      "[02:00.400 --> 02:01.660]  So this is it.\n",
      "[02:04.000 --> 02:06.760]  I mean, it's not like we have to show everything and everything.\n",
      "[02:10.840 --> 02:12.760]  No, no, no, it won't work like that.\n",
      "[02:13.460 --> 02:16.740]  It should be done in a consistent and sincere manner.\n",
      "[02:19.140 --> 02:20.500]  Nothing will happen by passing time.\n",
      "[02:21.360 --> 02:22.240]  We don't want to waste time.\n",
      "[02:24.040 --> 02:24.680]  Correct?\n",
      "[02:25.480 --> 02:28.640]  And then we have to include this module in our system,\n",
      "[02:28.960 --> 02:30.820]  or we have to include it in our business,\n",
      "[02:31.760 --> 02:33.000]  so that we can showcase all these projects.\n",
      "âœ… Translation complete!\n",
      "ğŸŒ en â†’ English\n",
      "ğŸ“ Segments: 36\n",
      "\n",
      "======================================================================\n",
      "STEP 2: SPEAKER DIARIZATION (PYANNOTE 3.1)\n",
      "======================================================================\n",
      "ğŸ¯ Loading Pyannote 3.1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79d33205a4c47cab51d65d2c2ba4395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/469 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9359b7f59945009bfd4d4cf0ff7f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a985306ab6e4f539e835b84f10f93b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7776f7e8ba49a7b0f1b273af2cfafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/26.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786a2e55a34f41a2895a638214e007af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/221 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Analyzing speakers...\n",
      "âœ… Detected 1 speakers\n",
      "\n",
      "======================================================================\n",
      "STEP 3: MERGING TRANSLATION WITH SPEAKERS\n",
      "======================================================================\n",
      "âœ… Merged 36 segments\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š TRANSCRIPT GENERATED\n",
      "======================================================================\n",
      "ğŸŒ Language: en â†’ English\n",
      "ğŸ‘¥ Speakers: 1\n",
      "â±ï¸  Duration: 2.6 minutes\n",
      "ğŸ“ Segments: 36\n",
      "\n",
      "ğŸ‘¥ Speaker participation:\n",
      "   Speaker 1: 36 segments (100.0%)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "TRANSCRIPT PREVIEW:\n",
      "----------------------------------------------------------------------\n",
      "[0:00:00] Speaker 1: Hello everyone, good morning. I hope I am audible to everyone. Today's topic of our discussion will be on business card detection. Right. So the agenda in discussing this project is that we have to detect the business card from a given video. Which means that in that video, we will be showing the business card. The person who has that business card will be showing it in the video, you will see that. And then, and then, I will be showing you the details in detail. So what this means is that we will be showing all the details on that business card, like the name of the card holder or the name of the business person, the name of the card, the company in which he is working, the designation the person is at, the address of the company, or the office, the phone number, the email address. So details like these needs to be extracted from the video. So we need to discuss on how can we do it. There are many ways that you can do normal Python coding, or you can use plain cod...\n",
      "\n",
      "======================================================================\n",
      "STEP 5: SUMMARIZATION WITH FLAN-T5-LARGE\n",
      "======================================================================\n",
      "ğŸ’» Device: cuda\n",
      "ğŸ¤– Loading google/flan-t5-large...\n",
      "   (This may take a few minutes...)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c14266ca9894c459ca7f38df830983e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123d17648e114846af16f7cf7528e6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cdb642328648a3b799c20d34bef919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff11ef1c64ff462091cf4f08c399f3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3095aa42f2d04389adab94998922cd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe08d4319e44c1cb6aef45d02fc12dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171a82d19c464678a803a13c4fd387b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully!\n",
      "ğŸ“Š Meeting: 2.6 min â†’ brief summary\n",
      "   Target length: 128-256 tokens\n",
      "â³ Generating summary...\n",
      "   This may take 2-5 minutes depending on transcript length...\n",
      "âœ… Summary generated!\n",
      "ğŸ“ Summary length: 117 words\n",
      "\n",
      "======================================================================\n",
      "ğŸ“‹ SUMMARY GENERATED\n",
      "======================================================================\n",
      "# MEETING SUMMARY\n",
      "**Duration**: 2.6 minutes\n",
      "**Speakers**: 1\n",
      "**Generated by**: FLAN-T5-Large\n",
      "\n",
      "---\n",
      "\n",
      "Today's topic of the meeting is business card detection. Speakers discussed how to extract business card details from a video and how to incorporate it into a system or in a business. Speaker 1: Hello everyone, good morning. I hope I am audible to everyone. Today'd topic of our discussion will be on business card recognition. Speaker 2: Today, we are going to discuss how to detect the business card ...\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ GENERATING BUSINESS DOCUMENTS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 6A: GENERATING BUSINESS REQUIREMENTS DOCUMENT\n",
      "======================================================================\n",
      "ğŸ¤– Loading FLAN-T5 for BRD generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Generating BRD... (this may take 2-3 minutes)\n",
      "âœ… BRD generated successfully!\n",
      "\n",
      "======================================================================\n",
      "STEP 6B: GENERATING PURCHASE ORDERS\n",
      "======================================================================\n",
      "ğŸ¤– Loading FLAN-T5 for PO generation...\n",
      "â³ Extracting purchase requirements...\n",
      "âœ… Purchase Order generated successfully!\n",
      "\n",
      "======================================================================\n",
      "ğŸ’¾ SAVING FILES\n",
      "======================================================================\n",
      "âœ… transcript_with_speakers.txt\n",
      "âœ… transcript_timestamps.txt\n",
      "âœ… meeting_summary.md\n",
      "âœ… business_requirements_document.md\n",
      "âœ… purchase_order_request.md\n",
      "âœ… complete_output.json\n",
      "\n",
      "======================================================================\n",
      "ğŸ‰ PIPELINE COMPLETE!\n",
      "======================================================================\n",
      "âœ… Translation: en â†’ English\n",
      "âœ… Speakers identified: 1\n",
      "âœ… Comprehensive summary generated\n",
      "âœ… Business Requirements Document created\n",
      "âœ… Purchase Order template generated\n",
      "âœ… All files saved successfully\n",
      "\n",
      "ğŸ“ Generated Documents:\n",
      "   1. transcript_with_speakers.txt - Clean transcript\n",
      "   2. transcript_timestamps.txt - With timestamps\n",
      "   3. meeting_summary.md - AI summary\n",
      "   4. business_requirements_document.md - BRD â­\n",
      "   5. purchase_order_request.md - PO template â­\n",
      "   6. complete_output.json - All data\n",
      "\n",
      "âœ… SUCCESS!\n",
      "   Original: en\n",
      "   Speakers: 1\n",
      "   Duration: 2.6 min\n",
      "\n",
      "ğŸ“ Check these files:\n",
      "   â€¢ transcript_with_speakers.txt\n",
      "   â€¢ transcript_timestamps.txt\n",
      "   â€¢ meeting_summary.md\n",
      "   â€¢ business_requirements_document.md â­ NEW!\n",
      "   â€¢ purchase_order_request.md â­ NEW!\n",
      "   â€¢ complete_output.json\n"
     ]
    }
   ],
   "source": [
    "# Complete Meeting Pipeline: Translation + Diarization + Summarization + BRD + PO\n",
    "# Whisper Large â†’ Pyannote 3.1 â†’ FLAN-T5-Large\n",
    "# Best quality at each stage\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "class Config:\n",
    "    # HuggingFace token for Pyannote\n",
    "    HUGGINGFACE_TOKEN = \"YOUR_HUGGINGFACE_TOKEN_HERE\"  # Get from https://huggingface.co/settings/tokens\n",
    "    \n",
    "    # Audio file\n",
    "    AUDIO_PATH = \"/kaggle/input/eng-hinbi-marathi-mix-audio/Random recording for ReqGen.m4a\"\n",
    "    \n",
    "    # Model settings\n",
    "    WHISPER_MODEL = \"large\"  # Translation model\n",
    "    SUMMARIZER_MODEL = \"google/flan-t5-large\"  # Summarization model\n",
    "    \n",
    "    # Speaker settings\n",
    "    MIN_SPEAKERS = None  # Auto-detect\n",
    "    MAX_SPEAKERS = None\n",
    "    \n",
    "    # Summarization settings\n",
    "    SUMMARY_MAX_LENGTH = 1024  # Max length of summary\n",
    "    SUMMARY_MIN_LENGTH = 256   # Min length of summary\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(f\"âš™ï¸  Configuration:\")\n",
    "print(f\"   Translation: Whisper {config.WHISPER_MODEL}\")\n",
    "print(f\"   Diarization: Pyannote 3.1 (DER: 11.2%)\")\n",
    "print(f\"   Summarization: {config.SUMMARIZER_MODEL}\")\n",
    "\n",
    "# ==================== AUDIO PREPARATION ====================\n",
    "def prepare_audio(audio_path):\n",
    "    \"\"\"Convert audio to WAV format\"\"\"\n",
    "    print(f\"\\nğŸ“ Loading audio: {audio_path}\")\n",
    "    \n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    audio = audio.set_channels(1).set_frame_rate(16000)\n",
    "    \n",
    "    wav_path = \"audio_processed.wav\"\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "    \n",
    "    duration_minutes = len(audio) / 1000 / 60\n",
    "    print(f\"â±ï¸  Duration: {duration_minutes:.2f} minutes\")\n",
    "    \n",
    "    return wav_path, duration_minutes\n",
    "\n",
    "# ==================== STEP 1: TRANSLATION ====================\n",
    "def translate_with_whisper(audio_path, model_size=\"large\"):\n",
    "    \"\"\"Translate audio to English with Whisper\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 1: TRANSLATION WITH WHISPER LARGE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"ğŸ’» Device: {device}\")\n",
    "    \n",
    "    print(f\"ğŸ¤ Loading Whisper {model_size}...\")\n",
    "    model = whisper.load_model(model_size, device=device)\n",
    "    \n",
    "    print(\"â³ Translating to English...\")\n",
    "    result = model.transcribe(\n",
    "        audio_path,\n",
    "        task=\"translate\",\n",
    "        word_timestamps=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Translation complete!\")\n",
    "    print(f\"ğŸŒ {result['language']} â†’ English\")\n",
    "    print(f\"ğŸ“ Segments: {len(result['segments'])}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ==================== STEP 2: DIARIZATION ====================\n",
    "def diarize_with_pyannote(audio_path, hf_token, min_speakers=None, max_speakers=None):\n",
    "    \"\"\"Identify speakers with Pyannote 3.1\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 2: SPEAKER DIARIZATION (PYANNOTE 3.1)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not hf_token or hf_token == \"YOUR_HF_TOKEN\":\n",
    "        print(\"âŒ ERROR: HuggingFace token required!\")\n",
    "        print(\"\\nAccept terms for ALL models:\")\n",
    "        print(\"   â€¢ https://huggingface.co/pyannote/speaker-diarization-3.1\")\n",
    "        print(\"   â€¢ https://huggingface.co/pyannote/segmentation-3.0\")\n",
    "        print(\"   â€¢ https://huggingface.co/pyannote/embedding\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ¯ Loading Pyannote 3.1...\")\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\",\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            pipeline.to(torch.device(\"cuda\"))\n",
    "        \n",
    "        print(\"â³ Analyzing speakers...\")\n",
    "        diarization = pipeline(\n",
    "            audio_path,\n",
    "            min_speakers=min_speakers,\n",
    "            max_speakers=max_speakers\n",
    "        )\n",
    "        \n",
    "        speaker_segments = []\n",
    "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            speaker_segments.append({\n",
    "                \"start\": turn.start,\n",
    "                \"end\": turn.end,\n",
    "                \"speaker\": speaker\n",
    "            })\n",
    "        \n",
    "        num_speakers = len(set([s[\"speaker\"] for s in speaker_segments]))\n",
    "        print(f\"âœ… Detected {num_speakers} speakers\")\n",
    "        \n",
    "        return speaker_segments\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==================== STEP 3: MERGE ====================\n",
    "def merge_translation_with_speakers(whisper_result, pyannote_segments):\n",
    "    \"\"\"Combine translation with speaker labels\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 3: MERGING TRANSLATION WITH SPEAKERS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not pyannote_segments:\n",
    "        print(\"âŒ No speaker segments\")\n",
    "        return None\n",
    "    \n",
    "    whisper_segments = whisper_result['segments']\n",
    "    merged_segments = []\n",
    "    \n",
    "    for w_seg in whisper_segments:\n",
    "        seg_start = w_seg['start']\n",
    "        seg_end = w_seg['end']\n",
    "        \n",
    "        assigned_speaker = \"UNKNOWN\"\n",
    "        max_overlap = 0\n",
    "        \n",
    "        for p_seg in pyannote_segments:\n",
    "            overlap_start = max(seg_start, p_seg['start'])\n",
    "            overlap_end = min(seg_end, p_seg['end'])\n",
    "            overlap = max(0, overlap_end - overlap_start)\n",
    "            \n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                assigned_speaker = p_seg['speaker']\n",
    "        \n",
    "        merged_segments.append({\n",
    "            'start': seg_start,\n",
    "            'end': seg_end,\n",
    "            'text': w_seg['text'].strip(),\n",
    "            'speaker': assigned_speaker\n",
    "        })\n",
    "    \n",
    "    # Rename speakers\n",
    "    speaker_counts = {}\n",
    "    for seg in merged_segments:\n",
    "        speaker = seg['speaker']\n",
    "        speaker_counts[speaker] = speaker_counts.get(speaker, 0) + 1\n",
    "    \n",
    "    sorted_speakers = sorted(speaker_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    speaker_mapping = {}\n",
    "    for i, (original_label, _) in enumerate(sorted_speakers, 1):\n",
    "        speaker_mapping[original_label] = f\"Speaker {i}\"\n",
    "    \n",
    "    for seg in merged_segments:\n",
    "        seg['speaker'] = speaker_mapping.get(seg['speaker'], seg['speaker'])\n",
    "    \n",
    "    print(f\"âœ… Merged {len(merged_segments)} segments\")\n",
    "    return merged_segments\n",
    "\n",
    "# ==================== STEP 4: FORMAT ====================\n",
    "def format_transcript(merged_segments):\n",
    "    \"\"\"Create readable transcript\"\"\"\n",
    "    lines_with_timestamps = []\n",
    "    lines_plain = []\n",
    "    \n",
    "    current_speaker = None\n",
    "    current_texts = []\n",
    "    segment_start_time = None\n",
    "    \n",
    "    for seg in merged_segments:\n",
    "        if seg['speaker'] != current_speaker:\n",
    "            if current_speaker:\n",
    "                timestamp = str(timedelta(seconds=int(segment_start_time)))\n",
    "                lines_with_timestamps.append(\n",
    "                    f\"[{timestamp}] {current_speaker}: {' '.join(current_texts)}\"\n",
    "                )\n",
    "            \n",
    "            current_speaker = seg['speaker']\n",
    "            current_texts = [seg['text']]\n",
    "            segment_start_time = seg['start']\n",
    "        else:\n",
    "            current_texts.append(seg['text'])\n",
    "    \n",
    "    if current_speaker:\n",
    "        timestamp = str(timedelta(seconds=int(segment_start_time)))\n",
    "        lines_with_timestamps.append(\n",
    "            f\"[{timestamp}] {current_speaker}: {' '.join(current_texts)}\"\n",
    "        )\n",
    "    \n",
    "    transcript_with_timestamps = \"\\n\\n\".join(lines_with_timestamps)\n",
    "    \n",
    "    # Plain format\n",
    "    current_speaker = None\n",
    "    current_texts = []\n",
    "    \n",
    "    for seg in merged_segments:\n",
    "        if seg['speaker'] != current_speaker:\n",
    "            if current_speaker:\n",
    "                lines_plain.append(f\"{current_speaker}: {' '.join(current_texts)}\")\n",
    "            current_speaker = seg['speaker']\n",
    "            current_texts = [seg['text']]\n",
    "        else:\n",
    "            current_texts.append(seg['text'])\n",
    "    \n",
    "    if current_speaker:\n",
    "        lines_plain.append(f\"{current_speaker}: {' '.join(current_texts)}\")\n",
    "    \n",
    "    transcript_plain = \"\\n\\n\".join(lines_plain)\n",
    "    \n",
    "    return transcript_with_timestamps, transcript_plain\n",
    "\n",
    "# ==================== STEP 5: SUMMARIZATION WITH FLAN-T5 ====================\n",
    "def summarize_with_flan_t5(transcript, duration_minutes, num_speakers, model_name=\"google/flan-t5-large\"):\n",
    "    \"\"\"\n",
    "    Generate comprehensive summary using FLAN-T5-Large\n",
    "    FLAN-T5 is instruction-tuned for better summarization quality\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 5: SUMMARIZATION WITH FLAN-T5-LARGE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"ğŸ’» Device: {device}\")\n",
    "    \n",
    "    print(f\"ğŸ¤– Loading {model_name}...\")\n",
    "    print(\"   (This may take a few minutes...)\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"âœ… Model loaded successfully!\")\n",
    "        \n",
    "        # Determine summary approach based on meeting length\n",
    "        if duration_minutes < 10:\n",
    "            summary_type = \"brief\"\n",
    "            max_length = 256\n",
    "            min_length = 128\n",
    "        elif duration_minutes < 30:\n",
    "            summary_type = \"moderate\"\n",
    "            max_length = 512\n",
    "            min_length = 256\n",
    "        elif duration_minutes < 60:\n",
    "            summary_type = \"detailed\"\n",
    "            max_length = 768\n",
    "            min_length = 384\n",
    "        else:\n",
    "            summary_type = \"comprehensive\"\n",
    "            max_length = 1024\n",
    "            min_length = 512\n",
    "        \n",
    "        print(f\"ğŸ“Š Meeting: {duration_minutes:.1f} min â†’ {summary_type} summary\")\n",
    "        print(f\"   Target length: {min_length}-{max_length} tokens\")\n",
    "        \n",
    "        # Create instruction-based prompt for FLAN-T5\n",
    "        instruction = f\"\"\"Analyze this {duration_minutes:.1f}-minute meeting transcript with {num_speakers} speakers and provide a {summary_type} summary.\n",
    "\n",
    "Your summary must include:\n",
    "1. Executive Summary: Brief overview of meeting purpose and outcome\n",
    "2. Key Discussion Points: Main topics discussed by each speaker\n",
    "3. Speaker Contributions: Notable points from each participant\n",
    "4. Decisions Made: Any decisions or conclusions reached\n",
    "5. Action Items: Tasks assigned to specific people\n",
    "6. Next Steps: Follow-up actions or future meetings\n",
    "\n",
    "Meeting Transcript:\n",
    "{transcript}\n",
    "\n",
    "Provide a well-structured, {summary_type} summary:\"\"\"\n",
    "\n",
    "        print(\"â³ Generating summary...\")\n",
    "        print(\"   This may take 2-5 minutes depending on transcript length...\")\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            instruction,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=2048,  # Input limit\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate summary\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                num_beams=4,  # Beam search for better quality\n",
    "                length_penalty=2.0,  # Encourage longer summaries\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,  # Avoid repetition\n",
    "                temperature=0.7,  # Some creativity\n",
    "                do_sample=False  # Deterministic for consistency\n",
    "            )\n",
    "        \n",
    "        # Decode summary\n",
    "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(\"âœ… Summary generated!\")\n",
    "        print(f\"ğŸ“ Summary length: {len(summary.split())} words\")\n",
    "        \n",
    "        # Format the summary with sections\n",
    "        formatted_summary = f\"\"\"# MEETING SUMMARY\n",
    "**Duration**: {duration_minutes:.1f} minutes\n",
    "**Speakers**: {num_speakers}\n",
    "**Generated by**: FLAN-T5-Large\n",
    "\n",
    "---\n",
    "\n",
    "{summary}\n",
    "\n",
    "---\n",
    "*Note: This summary was automatically generated using FLAN-T5-Large, a state-of-the-art instruction-tuned language model.*\n",
    "\"\"\"\n",
    "        \n",
    "        return formatted_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Summarization error: {e}\")\n",
    "        return f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "# ==================== STEP 6: GENERATE BRD & PURCHASE ORDERS ====================\n",
    "def generate_brd_from_summary(summary, transcript, duration_minutes, num_speakers):\n",
    "    \"\"\"\n",
    "    Generate Business Requirements Document from meeting summary\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 6A: GENERATING BUSINESS REQUIREMENTS DOCUMENT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ¤– Loading FLAN-T5 for BRD generation...\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(config.SUMMARIZER_MODEL)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(config.SUMMARIZER_MODEL)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Create BRD-specific prompt\n",
    "        brd_prompt = f\"\"\"You are a business analyst. Based on this meeting summary and transcript, create a professional Business Requirements Document (BRD).\n",
    "\n",
    "Meeting Summary:\n",
    "{summary}\n",
    "\n",
    "Meeting Transcript:\n",
    "{transcript[:2000]}\n",
    "\n",
    "Generate a structured BRD with these sections:\n",
    "1. DOCUMENT CONTROL (version, date, status)\n",
    "2. EXECUTIVE SUMMARY\n",
    "3. BUSINESS OBJECTIVES\n",
    "4. SCOPE (in-scope and out-of-scope items)\n",
    "5. STAKEHOLDERS AND ROLES\n",
    "6. FUNCTIONAL REQUIREMENTS (numbered list)\n",
    "7. NON-FUNCTIONAL REQUIREMENTS\n",
    "8. CONSTRAINTS AND ASSUMPTIONS\n",
    "9. SUCCESS CRITERIA\n",
    "10. RISKS AND DEPENDENCIES\n",
    "11. TIMELINE AND MILESTONES\n",
    "\n",
    "Create a detailed, professional BRD:\"\"\"\n",
    "\n",
    "        print(\"â³ Generating BRD... (this may take 2-3 minutes)\")\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            brd_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=2048,\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=1024,\n",
    "                min_length=512,\n",
    "                num_beams=4,\n",
    "                length_penalty=2.0,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        \n",
    "        brd_content = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Format BRD with proper structure\n",
    "        from datetime import datetime\n",
    "        today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        formatted_brd = f\"\"\"# BUSINESS REQUIREMENTS DOCUMENT (BRD)\n",
    "\n",
    "---\n",
    "\n",
    "## DOCUMENT CONTROL\n",
    "\n",
    "| Field | Value |\n",
    "|-------|-------|\n",
    "| **Document Title** | Business Requirements Document - Meeting Analysis |\n",
    "| **Version** | 1.0 |\n",
    "| **Date** | {today} |\n",
    "| **Status** | Draft |\n",
    "| **Meeting Duration** | {duration_minutes:.1f} minutes |\n",
    "| **Participants** | {num_speakers} speakers |\n",
    "| **Generated By** | FLAN-T5-Large AI System |\n",
    "\n",
    "---\n",
    "\n",
    "## DOCUMENT PURPOSE\n",
    "\n",
    "This Business Requirements Document captures the business needs, objectives, and requirements discussed in the meeting held on {today}. It serves as a formal record of what was agreed upon and what needs to be delivered.\n",
    "\n",
    "---\n",
    "\n",
    "{brd_content}\n",
    "\n",
    "---\n",
    "\n",
    "## APPROVAL SIGNATURES\n",
    "\n",
    "| Role | Name | Signature | Date |\n",
    "|------|------|-----------|------|\n",
    "| **Business Sponsor** | _______________ | _______________ | ______ |\n",
    "| **Project Manager** | _______________ | _______________ | ______ |\n",
    "| **Technical Lead** | _______________ | _______________ | ______ |\n",
    "| **Business Analyst** | _______________ | _______________ | ______ |\n",
    "\n",
    "---\n",
    "\n",
    "## REVISION HISTORY\n",
    "\n",
    "| Version | Date | Author | Description |\n",
    "|---------|------|--------|-------------|\n",
    "| 1.0 | {today} | AI System | Initial draft generated from meeting |\n",
    "\n",
    "---\n",
    "\n",
    "*This document was automatically generated using AI analysis of meeting transcript. Please review and validate all information before formal approval.*\n",
    "\"\"\"\n",
    "        \n",
    "        print(\"âœ… BRD generated successfully!\")\n",
    "        return formatted_brd\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ BRD generation error: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_purchase_orders(summary, transcript):\n",
    "    \"\"\"\n",
    "    Generate Purchase Orders from meeting discussion\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 6B: GENERATING PURCHASE ORDERS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ¤– Loading FLAN-T5 for PO generation...\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(config.SUMMARIZER_MODEL)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(config.SUMMARIZER_MODEL)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Create PO extraction prompt\n",
    "        po_prompt = f\"\"\"You are a procurement specialist. Analyze this meeting summary and transcript to identify any items, services, or resources that need to be purchased.\n",
    "\n",
    "Meeting Summary:\n",
    "{summary}\n",
    "\n",
    "Meeting Transcript:\n",
    "{transcript[:2000]}\n",
    "\n",
    "Extract and list:\n",
    "1. Items/services to be purchased\n",
    "2. Quantities (if mentioned)\n",
    "3. Estimated costs/budgets (if mentioned)\n",
    "4. Vendors/suppliers (if mentioned)\n",
    "5. Required delivery dates (if mentioned)\n",
    "6. Purpose/justification for each purchase\n",
    "7. Department/cost center responsible\n",
    "\n",
    "Format as a structured list of purchase requirements:\"\"\"\n",
    "\n",
    "        print(\"â³ Extracting purchase requirements...\")\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            po_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=2048,\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=768,\n",
    "                min_length=256,\n",
    "                num_beams=4,\n",
    "                length_penalty=2.0,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3\n",
    "            )\n",
    "        \n",
    "        po_content = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Format as professional PO document\n",
    "        from datetime import datetime, timedelta\n",
    "        today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        po_number = f\"PO-{datetime.now().strftime('%Y%m%d')}-001\"\n",
    "        delivery_date = (datetime.now() + timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        formatted_po = f\"\"\"# PURCHASE ORDER REQUEST\n",
    "\n",
    "---\n",
    "\n",
    "## PURCHASE ORDER DETAILS\n",
    "\n",
    "| Field | Value |\n",
    "|-------|-------|\n",
    "| **PO Number** | {po_number} |\n",
    "| **Date Issued** | {today} |\n",
    "| **Requested By** | [To be filled] |\n",
    "| **Department** | [To be filled] |\n",
    "| **Approval Status** | Pending Approval |\n",
    "| **Delivery Date Requested** | {delivery_date} |\n",
    "\n",
    "---\n",
    "\n",
    "## VENDOR INFORMATION\n",
    "\n",
    "| Field | Value |\n",
    "|-------|-------|\n",
    "| **Vendor Name** | [To be determined based on requirements] |\n",
    "| **Vendor Contact** | [To be filled] |\n",
    "| **Payment Terms** | Net 30 |\n",
    "| **Shipping Method** | [To be determined] |\n",
    "\n",
    "---\n",
    "\n",
    "## ITEMS/SERVICES REQUESTED\n",
    "\n",
    "{po_content}\n",
    "\n",
    "---\n",
    "\n",
    "## COST SUMMARY\n",
    "\n",
    "| Category | Estimated Amount |\n",
    "|----------|-----------------|\n",
    "| **Subtotal** | $[To be calculated] |\n",
    "| **Tax (if applicable)** | $[To be calculated] |\n",
    "| **Shipping** | $[To be calculated] |\n",
    "| **Total** | $[To be calculated] |\n",
    "\n",
    "---\n",
    "\n",
    "## BUSINESS JUSTIFICATION\n",
    "\n",
    "Based on the meeting discussion, these purchases are required to support:\n",
    "- Project objectives and deliverables\n",
    "- Timeline commitments\n",
    "- Resource needs identified by team\n",
    "- Strategic initiatives discussed\n",
    "\n",
    "---\n",
    "\n",
    "## BUDGET ALLOCATION\n",
    "\n",
    "| Cost Center | Budget Code | Amount |\n",
    "|-------------|-------------|--------|\n",
    "| [Department] | [Code] | $[Amount] |\n",
    "\n",
    "---\n",
    "\n",
    "## APPROVAL WORKFLOW\n",
    "\n",
    "| Level | Approver | Signature | Date | Status |\n",
    "|-------|----------|-----------|------|--------|\n",
    "| **Requestor** | [Name] | _______________ | ______ | Pending |\n",
    "| **Manager** | [Name] | _______________ | ______ | Pending |\n",
    "| **Finance** | [Name] | _______________ | ______ | Pending |\n",
    "| **Procurement** | [Name] | _______________ | ______ | Pending |\n",
    "\n",
    "---\n",
    "\n",
    "## TERMS AND CONDITIONS\n",
    "\n",
    "1. All items must meet specified quality standards\n",
    "2. Delivery must be completed by requested date\n",
    "3. Invoices should reference this PO number\n",
    "4. Any changes to this order must be approved in writing\n",
    "5. Payment will be processed upon receipt and verification of goods/services\n",
    "\n",
    "---\n",
    "\n",
    "## ADDITIONAL NOTES\n",
    "\n",
    "*This purchase order was automatically generated from meeting analysis. Please review all requirements carefully and fill in missing information before submitting for approval.*\n",
    "\n",
    "**Generated from meeting held on:** {today}\n",
    "**Source:** AI-analyzed meeting transcript\n",
    "\n",
    "---\n",
    "\n",
    "## ATTACHMENTS\n",
    "\n",
    "- Meeting transcript\n",
    "- Meeting summary\n",
    "- Business Requirements Document (if applicable)\n",
    "- Technical specifications (if applicable)\n",
    "\n",
    "---\n",
    "\n",
    "*For questions regarding this purchase order, please contact the requesting department or procurement team.*\n",
    "\"\"\"\n",
    "        \n",
    "        print(\"âœ… Purchase Order generated successfully!\")\n",
    "        return formatted_po\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ PO generation error: {e}\")\n",
    "        return None\n",
    "def complete_meeting_pipeline(\n",
    "    audio_path,\n",
    "    whisper_model=\"large\",\n",
    "    hf_token=None,\n",
    "    summarizer_model=\"google/flan-t5-large\",\n",
    "    min_speakers=None,\n",
    "    max_speakers=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete pipeline with best-in-class models at each stage\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸ™ï¸  COMPLETE MEETING ANALYSIS PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nğŸ† Best-in-class models:\")\n",
    "    print(\"   1. Whisper Large - Translation\")\n",
    "    print(\"   2. Pyannote 3.1 - Speaker Diarization (11.2% DER)\")\n",
    "    print(\"   3. FLAN-T5-Large - Comprehensive Summarization\")\n",
    "    print(\"\\nğŸ“‹ Pipeline stages:\")\n",
    "    print(\"   Stage 1: Translate audio to English\")\n",
    "    print(\"   Stage 2: Identify speakers with voice embeddings\")\n",
    "    print(\"   Stage 3: Merge translation with speaker labels\")\n",
    "    print(\"   Stage 4: Format readable transcript\")\n",
    "    print(\"   Stage 5: Generate comprehensive summary\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Prepare audio\n",
    "    wav_path, duration = prepare_audio(audio_path)\n",
    "    \n",
    "    # Stage 1: Translate\n",
    "    translation = translate_with_whisper(wav_path, whisper_model)\n",
    "    \n",
    "    # Stage 2: Diarize\n",
    "    speaker_segments = diarize_with_pyannote(\n",
    "        wav_path,\n",
    "        hf_token,\n",
    "        min_speakers,\n",
    "        max_speakers\n",
    "    )\n",
    "    \n",
    "    if not speaker_segments:\n",
    "        print(\"\\nâŒ Pipeline failed at diarization stage\")\n",
    "        return None\n",
    "    \n",
    "    # Stage 3: Merge\n",
    "    merged_segments = merge_translation_with_speakers(translation, speaker_segments)\n",
    "    \n",
    "    if not merged_segments:\n",
    "        print(\"\\nâŒ Pipeline failed at merging stage\")\n",
    "        return None\n",
    "    \n",
    "    # Stage 4: Format\n",
    "    transcript_ts, transcript_plain = format_transcript(merged_segments)\n",
    "    \n",
    "    num_speakers = len(set([s['speaker'] for s in merged_segments]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ“Š TRANSCRIPT GENERATED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"ğŸŒ Language: {translation['language']} â†’ English\")\n",
    "    print(f\"ğŸ‘¥ Speakers: {num_speakers}\")\n",
    "    print(f\"â±ï¸  Duration: {duration:.1f} minutes\")\n",
    "    print(f\"ğŸ“ Segments: {len(merged_segments)}\")\n",
    "    \n",
    "    # Display speaker stats\n",
    "    speaker_counts = {}\n",
    "    for seg in merged_segments:\n",
    "        speaker = seg['speaker']\n",
    "        speaker_counts[speaker] = speaker_counts.get(speaker, 0) + 1\n",
    "    \n",
    "    print(f\"\\nğŸ‘¥ Speaker participation:\")\n",
    "    for speaker in sorted(speaker_counts.keys(), key=lambda x: int(x.split()[1])):\n",
    "        count = speaker_counts[speaker]\n",
    "        percentage = (count / len(merged_segments)) * 100\n",
    "        print(f\"   {speaker}: {count} segments ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Display transcript preview\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"TRANSCRIPT PREVIEW:\")\n",
    "    print(\"-\" * 70)\n",
    "    preview = transcript_ts[:1000]\n",
    "    print(preview + (\"...\" if len(transcript_ts) > 1000 else \"\"))\n",
    "    \n",
    "    # Stage 5: Summarize\n",
    "    summary = summarize_with_flan_t5(\n",
    "        transcript_plain,\n",
    "        duration,\n",
    "        num_speakers,\n",
    "        config.SUMMARIZER_MODEL\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ“‹ SUMMARY GENERATED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(summary[:500] + (\"...\" if len(summary) > 500 else \"\"))\n",
    "    \n",
    "    # Stage 6A: Generate BRD\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ“„ GENERATING BUSINESS DOCUMENTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    brd = generate_brd_from_summary(\n",
    "        summary,\n",
    "        transcript_plain,\n",
    "        duration,\n",
    "        num_speakers\n",
    "    )\n",
    "    \n",
    "    # Stage 6B: Generate Purchase Orders\n",
    "    purchase_order = generate_purchase_orders(\n",
    "        summary,\n",
    "        transcript_plain\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ’¾ SAVING FILES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    output = {\n",
    "        \"original_language\": translation['language'],\n",
    "        \"translated_to\": \"English\",\n",
    "        \"duration_minutes\": duration,\n",
    "        \"num_speakers\": num_speakers,\n",
    "        \"models_used\": {\n",
    "            \"translation\": f\"Whisper {whisper_model}\",\n",
    "            \"diarization\": \"Pyannote 3.1 (DER: 11.2%)\",\n",
    "            \"summarization\": config.SUMMARIZER_MODEL\n",
    "        },\n",
    "        \"transcript_with_timestamps\": transcript_ts,\n",
    "        \"transcript_plain\": transcript_plain,\n",
    "        \"summary\": summary,\n",
    "        \"brd\": brd,\n",
    "        \"purchase_order\": purchase_order,\n",
    "        \"segments\": merged_segments\n",
    "    }\n",
    "    \n",
    "    with open(\"transcript_with_speakers.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(transcript_plain)\n",
    "    print(\"âœ… transcript_with_speakers.txt\")\n",
    "    \n",
    "    with open(\"transcript_timestamps.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(transcript_ts)\n",
    "    print(\"âœ… transcript_timestamps.txt\")\n",
    "    \n",
    "    with open(\"meeting_summary.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summary)\n",
    "    print(\"âœ… meeting_summary.md\")\n",
    "    \n",
    "    if brd:\n",
    "        with open(\"business_requirements_document.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(brd)\n",
    "        print(\"âœ… business_requirements_document.md\")\n",
    "    \n",
    "    if purchase_order:\n",
    "        with open(\"purchase_order_request.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(purchase_order)\n",
    "        print(\"âœ… purchase_order_request.md\")\n",
    "    \n",
    "    with open(\"complete_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    print(\"âœ… complete_output.json\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ‰ PIPELINE COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"âœ… Translation: {translation['language']} â†’ English\")\n",
    "    print(f\"âœ… Speakers identified: {num_speakers}\")\n",
    "    print(f\"âœ… Comprehensive summary generated\")\n",
    "    print(f\"âœ… Business Requirements Document created\")\n",
    "    print(f\"âœ… Purchase Order template generated\")\n",
    "    print(f\"âœ… All files saved successfully\")\n",
    "    print(\"\\nğŸ“ Generated Documents:\")\n",
    "    print(f\"   1. transcript_with_speakers.txt - Clean transcript\")\n",
    "    print(f\"   2. transcript_timestamps.txt - With timestamps\")\n",
    "    print(f\"   3. meeting_summary.md - AI summary\")\n",
    "    print(f\"   4. business_requirements_document.md - BRD â­\")\n",
    "    print(f\"   5. purchase_order_request.md - PO template â­\")\n",
    "    print(f\"   6. complete_output.json - All data\")\n",
    "    \n",
    "    # Cleanup\n",
    "    if os.path.exists(wav_path):\n",
    "        os.remove(wav_path)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# ==================== USAGE FUNCTIONS ====================\n",
    "\n",
    "def run_complete_pipeline():\n",
    "    \"\"\"Run full pipeline with all stages\"\"\"\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    \n",
    "    try:\n",
    "        hf_token = secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "    except:\n",
    "        print(\"âŒ HUGGINGFACE_TOKEN not found in Kaggle Secrets!\")\n",
    "        return None\n",
    "    \n",
    "    return complete_meeting_pipeline(\n",
    "        audio_path=config.AUDIO_PATH,\n",
    "        whisper_model=config.WHISPER_MODEL,\n",
    "        hf_token=hf_token,\n",
    "        summarizer_model=config.SUMMARIZER_MODEL,\n",
    "        min_speakers=config.MIN_SPEAKERS,\n",
    "        max_speakers=config.MAX_SPEAKERS\n",
    "    )\n",
    "\n",
    "def run_with_medium_models():\n",
    "    \"\"\"Faster processing with medium models\"\"\"\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    hf_token = secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "    \n",
    "    return complete_meeting_pipeline(\n",
    "        audio_path=config.AUDIO_PATH,\n",
    "        whisper_model=\"medium\",  # Faster\n",
    "        hf_token=hf_token,\n",
    "        summarizer_model=\"google/flan-t5-base\",  # Smaller, faster\n",
    "        min_speakers=config.MIN_SPEAKERS,\n",
    "        max_speakers=config.MAX_SPEAKERS\n",
    "    )\n",
    "\n",
    "def run_with_known_speakers(num_speakers):\n",
    "    \"\"\"When you know exact speaker count\"\"\"\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    hf_token = secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "    \n",
    "    return complete_meeting_pipeline(\n",
    "        audio_path=config.AUDIO_PATH,\n",
    "        whisper_model=config.WHISPER_MODEL,\n",
    "        hf_token=hf_token,\n",
    "        summarizer_model=config.SUMMARIZER_MODEL,\n",
    "        min_speakers=num_speakers,\n",
    "        max_speakers=num_speakers\n",
    "    )\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nğŸš€ Starting complete meeting analysis pipeline...\\n\")\n",
    "    \n",
    "    # Choose ONE:\n",
    "    \n",
    "    # Option 1: Full pipeline (RECOMMENDED)\n",
    "    result = run_complete_pipeline()\n",
    "    \n",
    "    # Option 2: Faster with medium models\n",
    "    # result = run_with_medium_models()\n",
    "    \n",
    "    # Option 3: Known speaker count\n",
    "    # result = run_with_known_speakers(num_speakers=3)\n",
    "    \n",
    "    if result:\n",
    "        print(f\"\\nâœ… SUCCESS!\")\n",
    "        print(f\"   Original: {result['original_language']}\")\n",
    "        print(f\"   Speakers: {result['num_speakers']}\")\n",
    "        print(f\"   Duration: {result['duration_minutes']:.1f} min\")\n",
    "        print(f\"\\nğŸ“ Check these files:\")\n",
    "        print(f\"   â€¢ transcript_with_speakers.txt\")\n",
    "        print(f\"   â€¢ transcript_timestamps.txt\")\n",
    "        print(f\"   â€¢ meeting_summary.md\")\n",
    "        print(f\"   â€¢ business_requirements_document.md â­ NEW!\")\n",
    "        print(f\"   â€¢ purchase_order_request.md â­ NEW!\")\n",
    "        print(f\"   â€¢ complete_output.json\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Pipeline failed. Check errors above.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8832408,
     "sourceId": 13972175,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31240,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
