{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13876892,"sourceType":"datasetVersion","datasetId":8832408}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers torch soundfile librosa","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install transformers torch torchaudio datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\nimport soundfile as sf\nimport librosa\n\n# Define the audio file path\nAUDIO_FILE_PATH = \"/kaggle/input/eng-hinbi-marathi-mix-audio/New Recording 220.m4a\"\n\n# --- Step 1: Transcribe the Audio using a Hugging Face ASR model ---\ntry:\n    # Use a robust ASR model like 'openai/whisper-base'\n    # 'whisper-large-v3' provides better accuracy but requires more memory/VRAM\n    print(f\"Loading ASR model (whisper-base)... This may take a moment the first time.\")\n    speech_to_text_pipeline = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\", chunk_length_s=30) # <-- Add this line to enable long-form transcription)\n\n    # Load and process the audio file\n    # Whisper models expect a 16kHz sampling rate\n    # librosa helps ensure the correct format\n    print(f\"Transcribing audio file: {AUDIO_FILE_PATH}\")\n    audio_input, sampling_rate = librosa.load(AUDIO_FILE_PATH, sr=16000)\n    \n    # Perform transcription\n    transcription_result = speech_to_text_pipeline(audio_input,return_timestamps=True)\n    meeting_transcript = transcription_result['text']\n    \n    \n    print(\"\\n--- Full Meeting Transcript ---\")\n    print(meeting_transcript)\n    print(\"=\"*50)\n\n    # --- Step 2: Summarize the Transcript using a Hugging Face Summarization model ---\n\n    # Use a summarization model like 'facebook/bart-large-cnn'\n    print(\"Loading Summarization model (bart-large-cnn)...\")\n    summarizer_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n    # Generate the summary from the transcribed text\n    # The max_length and min_length control the size of the generated summary\n    summary_result = summarizer_pipeline(\n        meeting_transcript,\n        max_length=150,\n        min_length=40,\n        do_sample=False\n    )\n\n    generated_summary = summary_result[0]['summary_text']\n\n    print(\"\\n--- AI-Generated Meeting Summary ---\")\n    print(generated_summary)\n    print(\"=\"*50 + \"\\n\")\n\nexcept FileNotFoundError:\n    print(f\"Error: Audio file not found at '{AUDIO_FILE_PATH}'\")\n    print(\"Please make sure you have an audio file named meeting_audio.wav in the current directory.\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n    print(\"Ensure all required libraries are installed and you have sufficient memory (RAM/VRAM) to run the models locally.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T07:06:34.689618Z","iopub.execute_input":"2025-11-27T07:06:34.689933Z","iopub.status.idle":"2025-11-27T07:08:47.570049Z","shell.execute_reply.started":"2025-11-27T07:06:34.689883Z","shell.execute_reply":"2025-11-27T07:08:47.569134Z"}},"outputs":[{"name":"stderr","text":"2025-11-27 07:06:49.868450: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764227210.119218      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764227210.184177      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Loading ASR model (whisper-base)... This may take a moment the first time.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"112eb5f9d7ff45499d4d5d48b8f6d8b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"506fd7849d0a4d0cbc3bd24399c2158a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"913d43995a7744a79a1737799f07759d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9c1b00a24de483daeca819a625896fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dac28547788b409aac69a857d7a09a5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1b34d253f2c40f09fbd1601d9257f96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d17be0ec01148809ad30ff9431bf2ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"883a6c67e3ca4b9f909c121d0f9fd6c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb456074babe4c1eb4b8eb03978fcb23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a3d695aecf442c3a1a611ed262e5e73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10534872ba17499c953561085bf5cc34"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\nUsing `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n","output_type":"stream"},{"name":"stdout","text":"Transcribing audio file: /kaggle/input/eng-hinbi-marathi-mix-audio/New Recording 220.m4a\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/2366572908.py:20: UserWarning: PySoundFile failed. Trying audioread instead.\n  audio_input, sampling_rate = librosa.load(AUDIO_FILE_PATH, sr=16000)\n/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n\tDeprecated as of librosa version 0.10.0.\n\tIt will be removed in librosa version 1.0.\n  y, sr_native = __audioread_load(path, offset, duration, dtype)\n/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:604: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\nUsing custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\nTranscription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Full Meeting Transcript ---\n So what was I saying is that tomorrow at 9 a.m., I have to go to the hospital ani mok tyanantar mala havde havai, dupari baaravajapar antamikorat hi hain. So, agi dho bhaji ki baad dho se agi me apna office ka kankar sakti hum.\n==================================================\nLoading Summarization model (bart-large-cnn)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee3b313524b34103a63b0ecb84644ba5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e07f02bd2b14fd1a849856e90091d08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8d3cc327ddd4989b87a569576948cac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"430f38d4334348ac8ad17ab2db298ba5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05db5866b11e4b8eb10389d480ace7b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4821dcc93b6487590c1a78fb08b7436"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\nYour max_length is set to 150, but your input_length is only 88. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n","output_type":"stream"},{"name":"stdout","text":"\n--- AI-Generated Meeting Summary ---\nTomorrow at 9 a.m., I have to go to the hospital ani mok tyanantar mala havde havai. So, agi dho bhaji ki baad dho se agi me apna office ka kankar sakti hum.\n==================================================\n\n","output_type":"stream"}],"execution_count":1}]}